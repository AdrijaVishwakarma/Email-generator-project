{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdrijaVishwakarma/Email-generator-project/blob/main/Email_generator_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hBa3hNZ1tT9D"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install transformers torch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MwK2BSGQICIB"
      },
      "outputs": [],
      "source": [
        "# Disable W&B logging and tokenizer warnings\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kYSj1afQICKj"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from transformers import pipeline, set_seed\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z-FsEK7ICNj",
        "outputId": "034798fb-629b-4a89-d62c-fa1ee672e691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: CPU\n"
          ]
        }
      ],
      "source": [
        "# Check GPU\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUsLySOeICQr",
        "outputId": "8ea95148-fe3f-40cb-9681-5ab4683d0430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "# Load model (lightweight for speed)\n",
        "MODEL_NAME = \"distilgpt2\"  # Change to \"gpt2\" for better quality\n",
        "print(\"Loading model...\")\n",
        "generator = pipeline(\"text-generation\", model=MODEL_NAME, device=device)\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "z6IbEUqdICT0"
      },
      "outputs": [],
      "source": [
        "# Function to build a generation prompt\n",
        "def build_prompt(purpose, recipient_name, key_points, tone=\"formal\"):\n",
        "    \"\"\"\n",
        "    Build a structured prompt for the AI model.\n",
        "    - purpose: what the email is about\n",
        "    - recipient_name: who you're writing to\n",
        "    - key_points: list or string of bullet points to include\n",
        "    - tone: formal / friendly / persuasive\n",
        "    \"\"\"\n",
        "    points_str = key_points if isinstance(key_points, str) else \"; \".join(key_points)\n",
        "    prompt = (\n",
        "        f\"Write a {tone} professional email to {recipient_name} about {purpose}.\\n\"\n",
        "        f\"Include these points: {points_str}.\\n\"\n",
        "        f\"Structure: subject line, greeting, body, and polite closing.\\n\\n\"\n",
        "        f\"Subject:\"\n",
        "    )\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "yGu-OfQ2ICWT"
      },
      "outputs": [],
      "source": [
        "# Example input\n",
        "purpose = \"following up on my M.Tech application\"\n",
        "recipient_name = \"Dr. Sharma (Admissions Committee)\"\n",
        "key_points = [\n",
        "    \"I applied on 10th June\",\n",
        "    \"I have research experience in VLSI and ML\",\n",
        "    \"I am available for interview this week\"\n",
        "]\n",
        "tone = \"formal\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Hy52kHj6ICb6"
      },
      "outputs": [],
      "source": [
        "# Build the prompt\n",
        "prompt = build_prompt(purpose, recipient_name, key_points, tone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Hn0I7P5EIY4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dab37c6-3b3b-4131-ad02-e39f06956663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        }
      ],
      "source": [
        "# Generate email\n",
        "output = generator(\n",
        "    prompt,\n",
        "    max_length=200,           # Total tokens in output\n",
        "    num_return_sequences=1,   # Number of different emails to return\n",
        "    temperature=0.8,          # Creativity (lower = more formal)\n",
        "    top_p=0.95,               # Nucleus sampling\n",
        "    do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "g_TgMSOEIY7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5edd559b-12fd-42ee-dd9a-aa8d94b8ee3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Generated Email ===\n",
            "Write a formal professional email to Dr. Sharma (Admissions Committee) about following up on my M.Tech application.\n",
            "Include these points: I applied on 10th June; I have research experience in VLSI and ML; I am available for interview this week.\n",
            "Structure: subject line, greeting, body, and polite closing.\n",
            "\n",
            "Subject:\n"
          ]
        }
      ],
      "source": [
        "# Display result\n",
        "print(\"=== Generated Email ===\")\n",
        "print(output[0][\"generated_text\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpztouL2OGpSGAkx+bdk0T",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}